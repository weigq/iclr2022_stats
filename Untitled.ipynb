{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15b9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import operator\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9030e10",
   "metadata": {},
   "source": [
    "## Get submission list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_url(page_id, driver, url_list):\n",
    "    # find all items in current page\n",
    "    # the first element is all submissions, the second is rejected/withdrawn ones\n",
    "    item_list_parent = driver.find_elements(By.CSS_SELECTOR, \"ul[class='list-unstyled submissions-list']\")[0]\n",
    "    item_list = item_list_parent.find_elements(By.CLASS_NAME, 'note')\n",
    "    item_list_len = len(item_list)\n",
    "    print(f'processing page {page_id} | {item_list_len} items | total: {len(url_list)} items')\n",
    "    for i in tqdm(range(item_list_len)):\n",
    "        # the fist <a> is the paper title and url\n",
    "        item = item_list[i].find_elements(By.TAG_NAME, 'a')[0] \n",
    "        url_list.append(item.get_attribute('href').strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc7f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(r'./chromedriver_win32/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.get('https://openreview.net/group?id=ICLR.cc/2022/Conference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b122854",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "page_id = 0\n",
    "is_not_end = True\n",
    "while(page_id >= 0):\n",
    "    if page_id == 0:\n",
    "        # process current page\n",
    "        get_item_url(page_id, driver, url_list)\n",
    "    else:\n",
    "        if is_not_end:\n",
    "            try:\n",
    "                # jump to next page\n",
    "                next_page_btns[0].find_element(By.TAG_NAME, 'a').click()\n",
    "                time.sleep(2.5)\n",
    "                # process current page\n",
    "                get_item_url(page_id, driver, url_list)\n",
    "            except:\n",
    "                print(f'Failed to jump to page {page_id}')\n",
    "        else:\n",
    "            break\n",
    "    next_page_btns = driver.find_elements(By.CSS_SELECTOR, \"li[class='  right-arrow']\")\n",
    "    is_not_end = len(next_page_btns) == 4\n",
    "    page_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fed47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save url list\n",
    "with open('assets/url_list.txt', 'w') as f:\n",
    "    f.write(time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime()) + '\\n')    \n",
    "    f.write('\\n'.join(url_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d75c2",
   "metadata": {},
   "source": [
    "## Parse each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4987b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission meta data\n",
    "class AllSubmissions:\n",
    "    def __init__(self, save_root: str):\n",
    "        self.items = []\n",
    "        self.save_root = save_root\n",
    "    \n",
    "    def update(self, index: int, url: str, title: str, keywords: list, scores: list, avg_score: float = -1.):\n",
    "        item = {}\n",
    "        if len(scores) > 0 and avg_score == -1.:\n",
    "            avg_score = np.mean(scores)    \n",
    "        item.update({\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'keywords': keywords,\n",
    "            'scores': scores,\n",
    "            'avg_score': avg_score\n",
    "        })\n",
    "        self.items.append(item)\n",
    "        with open(os.path.join(self.save_root, f'{index}.txt'), 'w') as f:\n",
    "            json.dump(item, f)\n",
    "    \n",
    "    def get_all_values_by_key(self, key: str = ''):\n",
    "        return list(map(operator.itemgetter(key), self.items))\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        with open(path, 'w') as f:\n",
    "            for item in self.items:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        print(f'Saved to {path}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d3d0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3328 items | time: 2021-11-09 17:11:59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read url list\n",
    "with open('assets/url_list.txt', 'r') as f:\n",
    "    url_list = f.readlines()\n",
    "data_time = url_list[0]\n",
    "item_list = url_list[1:]\n",
    "num_items = len(item_list)\n",
    "print(f'Total {num_items} items | time: {data_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8651f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ec1bcc13f54e7498b816ef20e60306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openreview.net/forum?id=B9LUI0pZFGc\n"
     ]
    }
   ],
   "source": [
    "all_submissions = AllSubmissions('assets/data/')\n",
    "for i in tqdm(range(num_items)):\n",
    "    item_url = item_list[i].strip()\n",
    "#     print(item_url)\n",
    "    driver.get(item_url)\n",
    "    time.sleep(1)\n",
    "    loaded = False\n",
    "    num_try = 0\n",
    "    while not loaded:\n",
    "        comment_list = driver.find_elements(By.CSS_SELECTOR, \"div[class='note_with_children comment-level-odd']\")\n",
    "        num_comment = len(comment_list)\n",
    "        if num_comment > 0:\n",
    "            loaded = True\n",
    "        else:\n",
    "            time.sleep(.5)\n",
    "            if num_try > 1000:\n",
    "                print(f'Failed to load {item_url} with max tries!')\n",
    "            num_try += 1\n",
    "    # process comments\n",
    "    item_scores = []\n",
    "    for comment in comment_list:\n",
    "        _comment = comment.find_elements(By.CLASS_NAME, 'meta_row')[0].find_elements(By.TAG_NAME, 'span')[0]\n",
    "        if 'ICLR' not in _comment.get_attribute('innerHTML'):\n",
    "            continue\n",
    "        if 'Reviewer' not in _comment.get_attribute('innerHTML'):\n",
    "            continue\n",
    "        _comment = comment.find_elements(By.CSS_SELECTOR, \"div[class='note panel']\")[0]\n",
    "        _comment = _comment.find_elements(By.CLASS_NAME, 'note_contents')\n",
    "        _comment = _comment[-2]\n",
    "        recommend = _comment.find_elements(By.TAG_NAME, 'span')\n",
    "        if recommend[0].get_attribute('innerHTML') == 'Recommendation: ':\n",
    "            _score = float(recommend[1].get_attribute('innerHTML').split(':')[0])\n",
    "            item_scores.append(_score)\n",
    "    # process title\n",
    "    _title = driver.find_elements(By.CLASS_NAME, 'note_content_title')[0].find_elements(By.TAG_NAME, 'a')[0]\n",
    "    item_title = _title.get_attribute('innerHTML').strip()\n",
    "    # process keywords\n",
    "    _keywords = driver.find_elements(By.CLASS_NAME, 'note_contents')[0]\n",
    "    if 'Keywords:' in _keywords.find_elements(By.TAG_NAME, 'span')[0].get_attribute('innerHTML'):\n",
    "        item_keywords = _keywords.find_elements(By.TAG_NAME, 'span')[1].get_attribute('innerHTML').strip().split(',')\n",
    "        item_keywords = [_k.strip() for _k in item_keywords]\n",
    "    else:\n",
    "        item_keywords = []\n",
    "    all_submissions.update(i, item_url, item_title, item_keywords, item_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
